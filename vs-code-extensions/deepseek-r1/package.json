{
  "name": "deep-seeker-ext",
  "displayName": "Deep-Seeker-ext",
  "publisher": "QusaiKagalwala",
  "description": "This VS Code extension allows users to run the DeepSeek 7B model locally using Ollama. With a simple and intuitive Command User Interface (CUI), users can download and interact with DeepSeek R1 directly within VS Code. Ideal for developers and AI enthusiasts, this extension provides an efficient way to run LLM-based tasks without relying on external APIs.",
  "version": "0.0.1",
  "engines": {
    "vscode": "^1.96.0"
  },
  "categories": [
    "Other"
  ],
  "activationEvents": [
    
  ],
  "main": "./out/extension.js",
  "contributes": {
    "commands": [
      {
        "command": "deep-seeker-ext.start",
        "title": "Chat with Deep-Seeker"
      }
    ]
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "pretest": "npm run compile && npm run lint",
    "lint": "eslint src",
    "test": "vscode-test"
  },
  "devDependencies": {
    "typescript": "^4.0.0",
    "eslint": "^7.0.0",
    "vscode-test": "^1.0.0",
    "@types/node": "^14.0.0",
    "@types/vscode": "^1.0.0",
    "@types/mocha": "^8.0.0",
    "@types/assert": "^1.0.0"
  },
  "dependencies": {
    "ollama": "^0.5.12"
  }
}
